{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1724230379558498448\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 9713889063787891886\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:00:56.451849: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-02-01 12:00:56.451875: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2024-02-01 12:00:56.451911: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2024-02-01 12:00:56.451952: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-02-01 12:00:56.451969: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import Counter\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Reshape, Dense, Dropout, GlobalAveragePooling2D, Input, Flatten, Conv2D,SeparableConv2D, BatchNormalization, Activation, MaxPooling2D, MaxPool2D, GlobalMaxPooling2D,Layer\n",
    "from tensorflow.keras.optimizers.legacy import Adam,RMSprop\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from  tensorflow.keras.utils   import plot_model\n",
    "import tqdm\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, Resizing\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load landmarks from CSV file\n",
    "def load_landmarks(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    landmarks = data.values\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(directory, batch_size=64, shuffle=True):\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        for file in os.listdir(class_dir):\n",
    "            file_paths.append(os.path.join(class_dir, file))\n",
    "            labels.append(class_name)\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    num_classes = len(np.unique(encoded_labels))\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Initialize the MinMaxScaler\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(file_paths))\n",
    "        else:\n",
    "            indices = np.arange(len(file_paths))\n",
    "        \n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            batch_input = []\n",
    "            batch_output = []\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                landmarks = load_landmarks(file_paths[idx])\n",
    "                landmarks_scaled = scaler.fit_transform(landmarks)\n",
    "                batch_input.append(landmarks_scaled)\n",
    "                batch_output.append(encoded_labels[idx])\n",
    "\n",
    "            batch_x = np.array(batch_input)\n",
    "            batch_y = to_categorical(batch_output, num_classes=num_classes)\n",
    "\n",
    "            yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(directory, batch_size=64, shuffle=True):\n",
    "#     file_paths = []\n",
    "#     labels = []\n",
    "#     for class_name in os.listdir(directory):\n",
    "#         class_dir = os.path.join(directory, class_name)\n",
    "#         for file in os.listdir(class_dir):\n",
    "#             file_paths.append(os.path.join(class_dir, file))\n",
    "#             labels.append(class_name)\n",
    "    \n",
    "#     label_encoder = LabelEncoder()\n",
    "#     encoded_labels = label_encoder.fit_transform(labels)\n",
    "#     num_classes = len(np.unique(encoded_labels))\n",
    "\n",
    "#     scaler = MinMaxScaler(feature_range=(0,1))  # Initialize the MinMaxScaler\n",
    "\n",
    "#     while True:\n",
    "#         if shuffle:\n",
    "#             indices = np.random.permutation(len(file_paths))\n",
    "#         else:\n",
    "#             indices = np.arange(len(file_paths))\n",
    "        \n",
    "#         for i in range(0, len(indices), batch_size):\n",
    "#             batch_indices = indices[i:i+batch_size]\n",
    "#             batch_input = []\n",
    "#             batch_output = []\n",
    "\n",
    "#             for idx in batch_indices:\n",
    "#                 landmarks = load_landmarks(file_paths[idx])\n",
    "#                 # Scale landmarks using MinMaxScaler\n",
    "#                 landmarks_scaled = scaler.fit_transform(landmarks)\n",
    "#                 batch_input.append(landmarks_scaled)\n",
    "#                 batch_output.append(encoded_labels[idx])\n",
    "\n",
    "#             batch_x = np.array(batch_input)\n",
    "#             batch_y = to_categorical(batch_output, num_classes=num_classes)\n",
    "\n",
    "#             yield batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count samples in each directory\n",
    "def count_samples(directory):\n",
    "    count = 0\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            count += len([f for f in os.listdir(class_dir) if f.endswith('.csv')])\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_dir = \"/Users/sanketdhameliya/Documents/CLBPSepDec2024---/ZulfiquarBhottoCLBPCodes/FER+_CSV_data/TRAIN\"\n",
    "valid_dir = \"/Users/sanketdhameliya/Documents/CLBPSepDec2024---/ZulfiquarBhottoCLBPCodes/FER+_CSV_data/VALID\"\n",
    "test_dir = \"/Users/sanketdhameliya/Documents/CLBPSepDec2024---/ZulfiquarBhottoCLBPCodes/FER+_CSV_data/VALID\"\n",
    "\n",
    "# Create data generators\n",
    "train_gen = data_generator(train_dir, batch_size=64, shuffle=True)  # Shuffle for training data\n",
    "valid_gen = data_generator(valid_dir, batch_size=64, shuffle=False)  # No shuffle for validation data\n",
    "test_gen = data_generator(test_dir, batch_size=64, shuffle=False)\n",
    "\n",
    "# Count the number of samples\n",
    "train_samples = count_samples(train_dir)\n",
    "validation_samples = count_samples(valid_dir)\n",
    "test_samples = count_samples(test_dir)\n",
    "\n",
    "# Batch size and steps calculation\n",
    "batch_size = 64\n",
    "steps_per_epoch = np.ceil(train_samples / batch_size)\n",
    "validation_steps = np.ceil(validation_samples / batch_size)\n",
    "test_steps = np.ceil(test_samples / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(31, 3, 1), name=\"img\")\n",
    "# # x = ZulfiquarAugmentationLayer(axis=1, q=3)(inputs)\n",
    "# x = layers.Conv2D(64, kernel_size=(3,3), strides = (1,1), padding='same',kernel_initializer=\"he_normal\",name=\"block1_conv1\")(inputs)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = ZulfiquarFFTLayer()(x)\n",
    "# x = layers.UpSampling2D(size=(2, 2), name=\"upsample1\")(x)\n",
    "# x = layers.Conv2D(64, kernel_size=(3,3), padding='same',kernel_initializer=\"he_normal\",name='block1_conv2')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = ZulfiquarReLuLayer()(x)  \n",
    "# x = layers.MaxPooling2D(2,2,name=\"maxpool_1\")(x)\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv1\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# x = layers.UpSampling2D(size=(2, 2), name=\"upsample_after_relu2\")(x) \n",
    "# #x = ZulfiquarReLuLayer()(x)\n",
    "# x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv2\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = ZulfiquarReLuLayer()(x)\n",
    "# # x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv3\")(x)\n",
    "# # x = layers.BatchNormalization()(x)\n",
    "# # x = layers.Activation('relu')(x)\n",
    "# # #x = ZulfiquarReLuLayer()(x) \n",
    "# x = layers.MaxPool2D(2,2, name=\"maxpool_2\")(x)\n",
    "# #x = ZulfiquarAugmentationLayer()(x)\n",
    "                                                                                                              \n",
    "# x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv1\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# x = layers.UpSampling2D(size=(2, 2), name=\"upsample_after_relu3\")(x)\n",
    "# #x = ZulfiquarReLuLayer()(x)\n",
    "# x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv2\")(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = ZulfiquarReLuLayer()(x)\n",
    "# # x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv3\")(x)\n",
    "# # x = layers.BatchNormalization()(x)\n",
    "# # x = layers.Activation('relu')(x)\n",
    "# x = layers.MaxPooling2D(1,1,name=\"maxpool_3\")(x)\n",
    "\n",
    "# x = layers.GlobalAveragePooling2D(name='GAP')(x)\n",
    "\n",
    "# #model.add(Flatten())\n",
    "# output = layers.Dense(7, activation='softmax', name = 'output')(x)\n",
    "\n",
    "# model = keras.Model(inputs, output, name=\"toy_resnet\")\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"toy_resnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmarks (InputLayer)      [(None, 36, 2, 1)]        0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 36, 2, 64)         640       \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 36, 2, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 36, 2, 64)         0         \n",
      "                                                                 \n",
      " maxpool_1 (MaxPooling2D)    (None, 18, 1, 64)         0         \n",
      "                                                                 \n",
      " zulfiquar_augmentation_lay  (None, 18, 2, 64)         0         \n",
      " er_1 (ZulfiquarAugmentatio                                      \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 18, 2, 96)         55392     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 18, 2, 96)         384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 18, 2, 96)         0         \n",
      "                                                                 \n",
      " upsample_2 (UpSampling2D)   (None, 36, 4, 96)         0         \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 36, 4, 96)         83040     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 36, 4, 96)         384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 36, 4, 96)         0         \n",
      "                                                                 \n",
      " block2_conv3 (Conv2D)       (None, 36, 4, 96)         83040     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 36, 4, 96)         384       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 36, 4, 96)         0         \n",
      "                                                                 \n",
      " maxpool_2 (MaxPooling2D)    (None, 18, 2, 96)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 18, 2, 128)        110720    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 18, 2, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 18, 2, 128)        0         \n",
      "                                                                 \n",
      " upsample_3 (UpSampling2D)   (None, 36, 4, 128)        0         \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 36, 4, 128)        147584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 36, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 36, 4, 128)        0         \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 36, 4, 128)        147584    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 36, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 36, 4, 128)        0         \n",
      "                                                                 \n",
      " maxpool_3 (MaxPooling2D)    (None, 18, 2, 128)        0         \n",
      "                                                                 \n",
      " GAP (GlobalAveragePooling2  (None, 128)               0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " output (Dense)              (None, 7)                 903       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 631847 (2.41 MB)\n",
      "Trainable params: 630375 (2.40 MB)\n",
      "Non-trainable params: 1472 (5.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(36, 2, 1), name=\"landmarks\")\n",
    "#x = ZulfiquarAugmentationLayer(axis=1)(inputs)\n",
    "x = layers.Conv2D(64, kernel_size=(3,3), strides = (1,1), padding='same',kernel_initializer=\"he_normal\",name=\"block1_conv1\")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "# x = layers.Conv2D(64, kernel_size=(3,3), padding='same',kernel_initializer=\"he_normal\",name='block1_conv2')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)  \n",
    "x = layers.MaxPooling2D(2,2,name=\"maxpool_1\")(x)\n",
    "x = ZulfiquarAugmentationLayer(axis=1, q=2)(x)\n",
    "\n",
    "x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv1\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.UpSampling2D(size=(2, 2), name=\"upsample_2\")(x)\n",
    "x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv2\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(96, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block2_conv3\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x) \n",
    "x = layers.MaxPool2D(2,2, name=\"maxpool_2\")(x)\n",
    "#x = ZulfiquarAugmentationLayer()(x)\n",
    "                                                                                                              \n",
    "\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv1\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.UpSampling2D(size=(2, 2), name=\"upsample_3\")(x)\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv2\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Conv2D(128, kernel_size=(3,3), padding=\"same\", kernel_initializer=\"he_normal\", name=\"block3_conv3\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling2D(2,2,name=\"maxpool_3\")(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D(name='GAP')(x)\n",
    "\n",
    "#model.add(Flatten())\n",
    "output = layers.Dense(7, activation='softmax', name = 'output')(x)\n",
    "\n",
    "model = keras.Model(inputs, output, name=\"toy_resnet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    #loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    loss=['categorical_crossentropy'],\n",
    "    #optimizer=keras.optimizers.RMSprop(),\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model_name = dump_path+\"fer_restnet1_weights.tf\"\n",
    "checkpoint = ModelCheckpoint(model_name, monitor = 'val_accuracy', verbose = 1, save_best_only = True, mode = 'max')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 12:01:05.345494: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=100,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[callbacks_list],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save history\n",
    "# np.save(dump_path + 'fer_restnet1_history.npy', history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(14, 6))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model Loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_accuracy = model.evaluate(test_gen, steps=test_steps)\n",
    "# print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_steps = int(np.ceil(validation_samples / batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_labels = []\n",
    "# predictions = []\n",
    "\n",
    "# # Ensure validation_steps is an integer\n",
    "# for _ in range(validation_steps):\n",
    "#     X_batch, y_batch = next(valid_gen)\n",
    "#     true_labels.extend(np.argmax(y_batch, axis=1))  # Store true labels\n",
    "#     preds = model.predict(X_batch)\n",
    "#     predictions.extend(np.argmax(preds, axis=1))  # Store predictions\n",
    "\n",
    "# # Convert lists to numpy arrays for further analysis\n",
    "# true_labels = np.array(true_labels)\n",
    "# predictions = np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data_and_encoder(directory, batch_size=64):\n",
    "#     file_paths = []\n",
    "#     labels = []\n",
    "#     for class_name in os.listdir(directory):\n",
    "#         class_dir = os.path.join(directory, class_name)\n",
    "#         for file in os.listdir(class_dir):\n",
    "#             file_paths.append(os.path.join(class_dir, file))\n",
    "#             labels.append(class_name)\n",
    "    \n",
    "#     label_encoder = LabelEncoder()\n",
    "#     label_encoder.fit(labels)  # Fit the encoder with all your class labels\n",
    "#     # Additional logic to prepare your data generator...\n",
    "\n",
    "#     return label_encoder  # Return the fitted label_encoder\n",
    "\n",
    "# # Use this function to get your label_encoder\n",
    "# label_encoder = prepare_data_and_encoder(train_dir, batch_size=64)\n",
    "# class_names = label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# # Assuming true_labels and predictions are already collected from the validation generator\n",
    "# cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# # Plotting the confusion matrix\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.show()\n",
    "\n",
    "# # Printing the classification report\n",
    "# print(classification_report(true_labels, predictions, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_learning_curves(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "#     epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "#     plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "#     plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(model_name)\n",
    "# results = model.evaluate(xag_test,y_test, verbose=2)\n",
    "# print(results)\n",
    "# print('Categorical accuracy, saved model {}%!'.format(\n",
    "#     round(results[1]*100,2)))\n",
    "# print('Crossentropy loss, saved model {}!'.format(\n",
    "#     round(results[0],2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
